{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MS1034/2021-CS-35-CVIP/blob/master/SNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Require Libraries**"
      ],
      "metadata": {
        "id": "ZYGwdOTBLuqN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IX5Vf-vALYjf",
        "outputId": "852dfa5c-b017-41ce-9c38-7d443a076fd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install pillow\n",
        "!pip install tqdm\n",
        "!pip install keras\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mount Drive**"
      ],
      "metadata": {
        "id": "hB9JEiltMB-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee6YEhORMFfU",
        "outputId": "14bd85bd-9d7b-4012-ae08-8bf9a5859ddc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ims3q0FwMrRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DataLoader(object):\n",
        "    \"\"\"\n",
        "    Class for loading data from image files\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, width, height, cells, data_path, output_path):\n",
        "        \"\"\"\n",
        "        Proper width and height for each image.\n",
        "        \"\"\"\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.cells = cells\n",
        "        self.data_path = data_path\n",
        "        self.output_path = output_path\n",
        "\n",
        "    def _open_image(self, path):\n",
        "        \"\"\"\n",
        "        Using the Image library we open the image in the given path. The path must lead to a .jpg file.\n",
        "        We then resize it to 105x105 like in the paper (the dataset contains 250x250 images.)\n",
        "\n",
        "        Returns the image as a numpy array.\n",
        "        \"\"\"\n",
        "        image = Image.open(path)\n",
        "        image = image.resize((self.width, self.height))\n",
        "        data = np.asarray(image)\n",
        "        data = np.array(data, dtype='float64')\n",
        "        return data\n",
        "\n",
        "    def convert_image_to_array(self, person, image_num, data_path, predict=False):\n",
        "        \"\"\"\n",
        "        Given a person, image number and datapath, returns a numpy array which represents the image.\n",
        "        predict - whether this function is called during training or testing. If called when training, we must reshape\n",
        "        the images since the given dataset is not in the correct dimensions.\n",
        "        \"\"\"\n",
        "        max_zeros = 4\n",
        "        image_num = '0' * max_zeros + image_num\n",
        "        image_num = image_num[-max_zeros:]\n",
        "        image_path = os.path.join(data_path, 'lfw2', person, f'{person}_{image_num}.jpg')\n",
        "        image_data = self._open_image(image_path)\n",
        "        if not predict:\n",
        "            image_data = image_data.reshape(self.width, self.height, self.cells)\n",
        "        return image_data\n",
        "\n",
        "    def load(self, set_name):\n",
        "        \"\"\"\n",
        "        Writes into the given output_path the images from the data_path.\n",
        "        dataset_type = train or test\n",
        "        \"\"\"\n",
        "        file_path = os.path.join(self.data_path, 'splits', f'{set_name}.txt')\n",
        "        print(file_path)\n",
        "        print('Loading dataset...')\n",
        "        x_first = []\n",
        "        x_second = []\n",
        "        y = []\n",
        "        names = []\n",
        "        with open(file_path, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "        for line in tqdm.tqdm(lines):\n",
        "            line = line.split()\n",
        "            if len(line) == 4:  # Class 0 - non-identical\n",
        "                names.append(line)\n",
        "                first_person_name, first_image_num, second_person_name, second_image_num = line[0], line[1], line[2], \\\n",
        "                                                                                           line[3]\n",
        "                first_image = self.convert_image_to_array(person=first_person_name,\n",
        "                                                          image_num=first_image_num,\n",
        "                                                          data_path=self.data_path)\n",
        "                second_image = self.convert_image_to_array(person=second_person_name,\n",
        "                                                           image_num=second_image_num,\n",
        "                                                           data_path=self.data_path)\n",
        "                x_first.append(first_image)\n",
        "                x_second.append(second_image)\n",
        "                y.append(0)\n",
        "            elif len(line) == 3:  # Class 1 - identical\n",
        "                names.append(line)\n",
        "                person_name, first_image_num, second_image_num = line[0], line[1], line[2]\n",
        "                first_image = self.convert_image_to_array(person=person_name,\n",
        "                                                          image_num=first_image_num,\n",
        "                                                          data_path=self.data_path)\n",
        "                second_image = self.convert_image_to_array(person=person_name,\n",
        "                                                           image_num=second_image_num,\n",
        "                                                           data_path=self.data_path)\n",
        "                x_first.append(first_image)\n",
        "                x_second.append(second_image)\n",
        "                y.append(1)\n",
        "            elif len(line) == 1:\n",
        "                print(f'line with a single value: {line}')\n",
        "        print('Done loading dataset')\n",
        "        with open(self.output_path, 'wb') as f:\n",
        "            pickle.dump([[x_first, x_second], y, names], f)\n",
        "\n",
        "\n",
        "print(\"Loaded data loader\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3TbgcK5Mq22",
        "outputId": "0805edb5-75ce-4284-fe5e-3fd22cc46110"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data loader\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W7rTUfPjNQHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import Input, Sequential, Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Lambda, BatchNormalization, Activation, \\\n",
        "    Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "\n",
        "class SiameseNetwork(object):\n",
        "    def __init__(self, seed, width, height, cells, loss, metrics, optimizer, dropout_rate):\n",
        "        \"\"\"\n",
        "        Seed - The seed used to initialize the weights\n",
        "        width, height, cells - used for defining the tensors used for the input images\n",
        "        loss, metrics, optimizer, dropout_rate - settings used for compiling the siamese model (e.g., 'Accuracy' and 'ADAM)\n",
        "        \"\"\"\n",
        "        K.clear_session()\n",
        "        self.load_file = None\n",
        "        self.seed = seed\n",
        "        self.initialize_seed()\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Define the matrices for the input images\n",
        "        input_shape = (width, height, cells)\n",
        "        left_input = Input(input_shape)\n",
        "        right_input = Input(input_shape)\n",
        "\n",
        "        # Get the CNN architecture as presented in the paper (read the readme for more information)\n",
        "        model = self._get_architecture(input_shape)\n",
        "        encoded_l = model(left_input)\n",
        "        encoded_r = model(right_input)\n",
        "\n",
        "        # Add a layer to combine the two CNNs\n",
        "        L1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
        "        L1_siamese_dist = L1_layer([encoded_l, encoded_r])\n",
        "        L1_siamese_dist = Dropout(dropout_rate)(L1_siamese_dist)\n",
        "\n",
        "        # An output layer with Sigmoid activation function\n",
        "        prediction = Dense(1, activation='sigmoid', bias_initializer=self.initialize_bias)(L1_siamese_dist)\n",
        "\n",
        "        siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
        "        self.siamese_net = siamese_net\n",
        "        self.siamese_net.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "    def initialize_seed(self):\n",
        "        \"\"\"\n",
        "        Initialize seed all for environment\n",
        "        \"\"\"\n",
        "        os.environ['PYTHONHASHSEED'] = str(self.seed)\n",
        "        random.seed(self.seed)\n",
        "        np.random.seed(self.seed)\n",
        "        tf.random.set_seed(self.seed)\n",
        "\n",
        "    def initialize_weights(self, shape, dtype=None):\n",
        "        \"\"\"\n",
        "        Called when initializing the weights of the siamese model, uses the random_normal function of keras to return a\n",
        "        tensor with a normal distribution of weights.\n",
        "        \"\"\"\n",
        "        return K.random_normal(shape, mean=0.0, stddev=0.01, dtype=dtype, seed=self.seed)\n",
        "\n",
        "    def initialize_bias(self, shape, dtype=None):\n",
        "        \"\"\"\n",
        "        Called when initializing the biases of the siamese model, uses the random_normal function of keras to return a\n",
        "        tensor with a normal distribution of weights.\n",
        "        \"\"\"\n",
        "        return K.random_normal(shape, mean=0.5, stddev=0.01, dtype=dtype, seed=self.seed)\n",
        "\n",
        "    def _get_architecture(self, input_shape):\n",
        "        \"\"\"\n",
        "        Returns a Convolutional Neural Network based on the input shape given of the images. This is the CNN network\n",
        "        that is used inside the siamese model. Uses parameters from the siamese one shot paper.\n",
        "        \"\"\"\n",
        "        model = Sequential()\n",
        "        model.add(\n",
        "            Conv2D(filters=64,\n",
        "                   kernel_size=(10, 10),\n",
        "                   input_shape=input_shape,\n",
        "                   kernel_initializer=self.initialize_weights,\n",
        "                   kernel_regularizer=l2(2e-4),\n",
        "                   name='Conv1'\n",
        "                   ))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D())\n",
        "\n",
        "        model.add(\n",
        "            Conv2D(filters=128,\n",
        "                   kernel_size=(7, 7),\n",
        "                   kernel_initializer=self.initialize_weights,\n",
        "                   bias_initializer=self.initialize_bias,\n",
        "                   kernel_regularizer=l2(2e-4),\n",
        "                   name='Conv2'\n",
        "                   ))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D())\n",
        "\n",
        "        model.add(\n",
        "            Conv2D(filters=128,\n",
        "                   kernel_size=(4, 4),\n",
        "                   kernel_initializer=self.initialize_weights,\n",
        "                   bias_initializer=self.initialize_bias,\n",
        "                   kernel_regularizer=l2(2e-4),\n",
        "                   name='Conv3'\n",
        "                   ))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D())\n",
        "\n",
        "        model.add(\n",
        "            Conv2D(filters=256,\n",
        "                   kernel_size=(4, 4),\n",
        "                   kernel_initializer=self.initialize_weights,\n",
        "                   bias_initializer=self.initialize_bias,\n",
        "                   kernel_regularizer=l2(2e-4),\n",
        "                   name='Conv4'\n",
        "                   ))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation(\"relu\"))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(\n",
        "            Dense(4096,\n",
        "                  activation='sigmoid',\n",
        "                  kernel_initializer=self.initialize_weights,\n",
        "                  kernel_regularizer=l2(2e-3),\n",
        "                  bias_initializer=self.initialize_bias))\n",
        "        return model\n",
        "\n",
        "    def _load_weights(self, weights_file):\n",
        "        \"\"\"\n",
        "        A function that attempts to load pre-existing weight files for the siamese model. If it succeeds then returns\n",
        "        True and updates the weights, otherwise False.\n",
        "        :return True if the file is already exists\n",
        "        \"\"\"\n",
        "        # self.siamese_net.summary()\n",
        "        self.load_file = weights_file\n",
        "        if os.path.exists(weights_file):  # if the file is already exists, load and return true\n",
        "            print('Loading pre-existed weights file')\n",
        "            self.siamese_net.load_weights(weights_file)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def fit(self, weights_file, train_path, validation_size, batch_size, epochs, early_stopping, patience, min_delta):\n",
        "        \"\"\"\n",
        "        Function for fitting the model. If the weights already exist, just return the summary of the model. Otherwise,\n",
        "        perform a whole train/validation/test split and train the model with the given parameters.\n",
        "        \"\"\"\n",
        "        print(train_path)\n",
        "        with open(train_path, 'rb') as f:\n",
        "            x_train, y_train, names = pickle.load(f)\n",
        "            print(f\"Trining Length:{len(x_train[0])}\")\n",
        "        \"\"\"\n",
        "        X_train[0]:  |----------x_train_0---------------------------|-------x_val_0--------|\n",
        "        X_train[1]:  |----------x_train_1---------------------------|-------x_val_1--------|\n",
        "        y_train:     |----------y_train_0 = y_train_1---------------|----y_val_0=y_val_1---|\n",
        "        \"\"\"\n",
        "        x_train_0, x_val_0, y_train_0, y_val_0 = train_test_split(x_train[0], y_train,\n",
        "                                                                  test_size=validation_size,\n",
        "                                                                  random_state=self.seed)\n",
        "        x_train_1, x_val_1, y_train_1, y_val_1 = train_test_split(x_train[1], y_train,\n",
        "                                                                  test_size=validation_size,\n",
        "                                                                  random_state=self.seed)\n",
        "        x_train_0 = np.array(x_train_0, dtype='float64')\n",
        "        x_val_0 = np.array(x_val_0, dtype='float64')\n",
        "        x_train_1 = np.array(x_train_1, dtype='float64')\n",
        "        x_val_1 = np.array(x_val_1, dtype='float64')\n",
        "        x_train = [x_train_0, x_train_1]\n",
        "        x_val = [x_val_0, x_val_1]\n",
        "        if y_train_0 != y_train_1 and y_val_0 != y_val_1:\n",
        "            raise Exception(\"y train lists or y validation list do not equal\")\n",
        "        y_train_both = np.array(y_train_0, dtype='float64')\n",
        "        y_val_both = np.array(y_val_0, dtype='float64')\n",
        "        if not self._load_weights(weights_file=weights_file):\n",
        "            print('No such pre-existed weights file')\n",
        "            print('Beginning to fit the model')\n",
        "            callback = []\n",
        "            if early_stopping:\n",
        "                \"\"\"\n",
        "                We used the EarlyStopping function monitoring on the validation loss with a minimum delta of 0.1\n",
        "                (Minimum change in the monitored quantity to qualify as an improvement, i.e.\n",
        "                an absolute change of less than min_delta, will count as no improvement.) and patience 5\n",
        "                (Number of epochs with no improvement after which training will be stopped.).\n",
        "                The direction is automatically inferred from the name of the monitored quantity (‘auto’).\n",
        "                \"\"\"\n",
        "                es = EarlyStopping(monitor='val_loss', min_delta=min_delta, patience=patience, mode='auto', verbose=1)\n",
        "                callback.append(es)\n",
        "            self.siamese_net.fit(x_train, y_train_both, batch_size=batch_size, epochs=epochs,\n",
        "                                 validation_data=(x_val, y_val_both), callbacks=callback, verbose=1)\n",
        "            self.siamese_net.save_weights(self.load_file)\n",
        "        # evaluate on the testing set\n",
        "        loss, accuracy = self.siamese_net.evaluate(x_val, y_val_both, batch_size=batch_size)\n",
        "\n",
        "        print(f'Loss on Validation set: {loss}')\n",
        "        print(f'Accuracy on Validation set: {accuracy}')\n",
        "\n",
        "    def evaluate(self, test_file, batch_size, analyze=False):\n",
        "        \"\"\"\n",
        "        Function for evaluating the final model after training.\n",
        "        test_file - file path to the test file.\n",
        "        batch_size - the batch size used in training.\n",
        "\n",
        "        Returns the loss and accuracy results.\n",
        "        \"\"\"\n",
        "        with open(test_file, 'rb') as f:\n",
        "            x_test, y_test, names = pickle.load(f)\n",
        "        print(f'Available Metrics: {self.siamese_net.metrics_names}')\n",
        "        y_test = np.array(y_test, dtype='float64')\n",
        "        x_test[0] = np.array(x_test[0], dtype='float64')\n",
        "        x_test[1] = np.array(x_test[1], dtype='float64')\n",
        "        # evaluate on the test set\n",
        "        loss, accuracy = self.siamese_net.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "\n",
        "        y_pred = self.siamese_net.predict(x_test)\n",
        "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "        report = classification_report(y_test, y_pred_binary)\n",
        "        print(\"Classification Report:\")\n",
        "        print(report)\n",
        "        if analyze:\n",
        "            self._analyze(x_test, y_test, names)\n",
        "        return loss, accuracy\n",
        "\n",
        "    def _analyze(self, x_test, y_test, names):\n",
        "        \"\"\"\n",
        "        Function used for evaluating our network in the methods proposed in the assignment.\n",
        "        We will find:\n",
        "        - The person who has 2 images that are the most dissimilar to each other\n",
        "        - The person with the two images that are the most similar to each other\n",
        "        - Two people with the most dissimilar images, and\n",
        "        - The two people with the most similar images.\n",
        "        \"\"\"\n",
        "        best_class_0_prob = 1  # correct classification for different people, y=0, prediction->0\n",
        "        best_class_0_name = None\n",
        "        worst_class_0_prob = 0  # misclassification for different people, y=0, prediction->1\n",
        "        worst_class_0_name = None\n",
        "        best_class_1_prob = 0  # correct classification for same people, y=1, prediction->1\n",
        "        best_class_1_name = None\n",
        "        worst_class_1_prob = 1  # misclassification for same people, y=1, prediction->0\n",
        "        worst_class_1_name = None\n",
        "        prob = self.siamese_net.predict(x_test)\n",
        "        for pair_index in range(len(names)):\n",
        "            name = names[pair_index]\n",
        "            y_pair = y_test[pair_index]\n",
        "            pair_prob = prob[pair_index][0]\n",
        "            if y_pair == 0:  # different people (actual)\n",
        "                if pair_prob < best_class_0_prob:  # correct classification for different people, y=0, prediction->0\n",
        "                    best_class_0_prob = pair_prob\n",
        "                    best_class_0_name = name\n",
        "                if pair_prob > worst_class_0_prob:  # misclassification for different people, y=0, prediction->1\n",
        "                    worst_class_0_prob = pair_prob\n",
        "                    worst_class_0_name = name\n",
        "            else:  # the same person (actual)\n",
        "                if pair_prob > best_class_1_prob:  # correct classification for same people, y=1, prediction->1\n",
        "                    best_class_1_prob = pair_prob\n",
        "                    best_class_1_name = name\n",
        "                if pair_prob < worst_class_1_prob:  # misclassification for same people, y=1, prediction->0\n",
        "                    worst_class_1_prob = pair_prob\n",
        "                    worst_class_1_name = name\n",
        "\n",
        "        print(f'correct classification for different people, y=0, prediction->0, name: {best_class_0_name} | prob: {best_class_0_prob}')\n",
        "        print(f'misclassification for different people, y=0, prediction->1, name: {worst_class_0_name} | prob: {worst_class_0_prob}')\n",
        "        print(f'correct classification for same people, y=1, prediction->1, name: {best_class_1_name} | prob: {best_class_1_prob}')\n",
        "        print(f'misclassification for same people, y=1, prediction->0, name: {worst_class_1_name} | prob: {worst_class_1_prob}')\n",
        "\n",
        "\n",
        "print(\"Loaded Siamese Network\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhvwtbGzNPmr",
        "outputId": "05de2bd8-0546-4df6-e165-512ab0d9d79b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Siamese Network\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOa3RNj2OzU9",
        "outputId": "0912748b-f045-4aae-aabe-133509d429b8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HrHPT2lTMwua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "path_separator = os.path.sep\n",
        "# Environment settings\n",
        "IS_COLAB = (os.name == 'posix')\n",
        "LOAD_DATA = not (os.name == 'posix')\n",
        "IS_EXPERIMENT = False\n",
        "train_name = 'train'\n",
        "test_name = 'test'\n",
        "WIDTH = HEIGHT = 105\n",
        "CEELS = 1\n",
        "loss_type = \"binary_crossentropy\"\n",
        "validation_size = 0.2\n",
        "early_stopping = True\n",
        "\n",
        "if IS_COLAB:\n",
        "    # the google drive folder we used\n",
        "    data_path = os.path.sep + os.path.join('content', 'drive', 'My\\ Drive', 'datasets', 'lfw2').replace('\\\\', '')\n",
        "else:\n",
        "    # locally\n",
        "    from data_loader import DataLoader\n",
        "    from siamese_network import SiameseNetwork\n",
        "\n",
        "    data_path = os.path.join('lfwa', 'lfw2')\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "\n",
        "def run_combination(l, bs, ep, pat, md, seed, train_path, test_path):\n",
        "    \"\"\"\n",
        "    This function gets the parameters and run the experiment.\n",
        "    :return: loss - loss on the testing set, accuracy - accuracy on the testing set\n",
        "    \"\"\"\n",
        "    # file types\n",
        "    model_save_type = 'h5'\n",
        "    # files paths\n",
        "    initialize_seed(seed)\n",
        "    parameters_name = f'seed_{seed}_lr_{l}_bs_{bs}_ep_{ep}_val_{validation_size}_' \\\n",
        "                      f'es_{early_stopping}_pa_{pat}_md_{md}'\n",
        "    print(f'Running combination with {parameters_name}')\n",
        "    # A path for the weights\n",
        "    load_weights_path = os.path.join(data_path, 'weights', f'weights_{parameters_name}.{model_save_type}')\n",
        "\n",
        "    siamese = SiameseNetwork(seed=seed, width=WIDTH, height=HEIGHT, cells=CEELS, loss=loss_type, metrics=['accuracy'],\n",
        "                             optimizer=Adam(lr=l), dropout_rate=0.4)\n",
        "    siamese.fit(weights_file=load_weights_path, train_path=train_path, validation_size=validation_size,\n",
        "                batch_size=bs, epochs=ep, early_stopping=early_stopping, patience=pat,\n",
        "                min_delta=md)\n",
        "    loss, accuracy = siamese.evaluate(test_file=test_path, batch_size=bs, analyze=True)\n",
        "    print(f'Loss on Testing set: {loss}')\n",
        "    print(f'Accuracy on Testing set: {accuracy}')\n",
        "\n",
        "    # predict_pairs(model)\n",
        "    return loss, accuracy\n",
        "\n",
        "\n",
        "def run():\n",
        "    \"\"\"\n",
        "    The main function that runs the training and experiments. Uses the global variables above.\n",
        "    \"\"\"\n",
        "    # file types\n",
        "    data_set_save_type = 'pickle'\n",
        "    train_path = os.path.join(data_path, f'{train_name}.{data_set_save_type}')  # A path for the train file\n",
        "    test_path = os.path.join(data_path, f'{test_name}.{data_set_save_type}')  # A path for the test file\n",
        "    if LOAD_DATA:  # If the training data already exists\n",
        "        loader = DataLoader(width=WIDTH, height=HEIGHT, cells=CEELS, data_path=data_path, output_path=train_path)\n",
        "        loader.load(set_name=train_name)\n",
        "        loader = DataLoader(width=WIDTH, height=HEIGHT, cells=CEELS, data_path=data_path, output_path=test_path)\n",
        "        loader.load(set_name=test_name)\n",
        "\n",
        "    result_path = os.path.join(data_path, f'results.csv')  # A path for the train file\n",
        "    results = {'lr': [], 'batch_size': [], 'epochs': [], 'patience': [], 'min_delta': [], 'seed': [], 'loss': [],\n",
        "               'accuracy': []}\n",
        "    for l in lr:\n",
        "        for bs in batch_size:\n",
        "            for ep in epochs:\n",
        "                for pat in patience:\n",
        "                    for md in min_delta:\n",
        "                        for seed in seeds:\n",
        "                            loss, accuracy = run_combination(l=l, bs=bs, ep=ep, pat=pat, md=md, seed=seed,\n",
        "                                                             train_path=train_path, test_path=test_path)\n",
        "                            results['lr'].append(l)\n",
        "                            results['batch_size'].append(bs)\n",
        "                            results['epochs'].append(ep)\n",
        "                            results['patience'].append(pat)\n",
        "                            results['min_delta'].append(md)\n",
        "                            results['seed'].append(seed)\n",
        "                            results['loss'].append(loss)\n",
        "                            results['accuracy'].append(accuracy)\n",
        "    df_results = pd.DataFrame.from_dict(results)\n",
        "    df_results.to_csv(result_path)\n",
        "\n",
        "\n",
        "def initialize_seed(seed):\n",
        "    \"\"\"\n",
        "    Initialize all relevant environments with the seed.\n",
        "    \"\"\"\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if IS_EXPERIMENT:\n",
        "        # Experiments settings\n",
        "        seeds = [0]\n",
        "        lr = [0.00005]\n",
        "        batch_size = [5]\n",
        "        epochs = [5]\n",
        "        patience = [5]\n",
        "        min_delta = [0.1]\n",
        "    else:\n",
        "        # Final settings\n",
        "        seeds = [0]\n",
        "        lr = [0.00005]\n",
        "        batch_size = [5]\n",
        "        epochs = [5]\n",
        "        patience = [5]\n",
        "        min_delta = [0.1]\n",
        "\n",
        "    print(os.name)\n",
        "    start_time = time.time()\n",
        "    print('Starting the experiments')\n",
        "    run()\n",
        "    print(f'Total Running Time: {time.time() - start_time}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYyOP48sMwTe",
        "outputId": "b427be84-6f41-4477-ffde-266124691001"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "posix\n",
            "Starting the experiments\n",
            "Running combination with seed_0_lr_5e-05_bs_5_ep_5_val_0.2_es_True_pa_5_md_0.1\n",
            "/content/drive/My Drive/datasets/lfw2/train.pickle\n",
            "Trining Length:2200\n",
            "Loading pre-existed weights file\n",
            "88/88 [==============================] - 2s 8ms/step - loss: 1.7150 - accuracy: 0.6523\n",
            "Loss on Validation set: 1.7150386571884155\n",
            "Accuracy on Validation set: 0.6522727012634277\n",
            "Available Metrics: ['loss', 'accuracy']\n",
            "200/200 [==============================] - 2s 7ms/step - loss: 1.7160 - accuracy: 0.6600\n",
            "32/32 [==============================] - 1s 12ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.53      0.61       500\n",
            "         1.0       0.63      0.79      0.70       500\n",
            "\n",
            "    accuracy                           0.66      1000\n",
            "   macro avg       0.67      0.66      0.65      1000\n",
            "weighted avg       0.67      0.66      0.65      1000\n",
            "\n",
            "32/32 [==============================] - 0s 12ms/step\n",
            "correct classification for different people, y=0, prediction->0, name: ['Jose_Bove', '1', 'Michalis_Chrisohoides', '1'] | prob: 0.10458244383335114\n",
            "misclassification for different people, y=0, prediction->1, name: ['Bob_Iger', '1', 'Brian_Cowen', '1'] | prob: 0.8832272291183472\n",
            "correct classification for same people, y=1, prediction->1, name: ['John_Garamendi', '1', '2'] | prob: 0.829390823841095\n",
            "misclassification for same people, y=1, prediction->0, name: ['Jennifer_Capriati', '3', '13'] | prob: 0.1742158830165863\n",
            "Loss on Testing set: 1.716011643409729\n",
            "Accuracy on Testing set: 0.6600000262260437\n",
            "Total Running Time: 13.276731252670288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "def load_pickle_file(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "train_name = 'train'\n",
        "\n",
        "data_path = os.path.sep + os.path.join('content', 'drive', 'My\\ Drive', 'datasets', 'lfw2').replace('\\\\', '')\n",
        "data_set_save_type = 'pickle'\n",
        "train_path = os.path.join(data_path, f'{train_name}.{data_set_save_type}')\n",
        "print(train_path)\n",
        "data = load_pickle_file(train_path)\n",
        "\n",
        "# Now you can inspect the loaded data\n",
        "print(data)\n",
        "print(\"I Love u\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSJ5ejfAZWkv",
        "outputId": "0ce271cd-a473-4985-86c3-8facc3d521fd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/datasets/lfw2/train.pickle\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}